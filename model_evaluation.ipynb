{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Env.MariNav import *\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sb3_contrib import MaskablePPO\n",
    "import h3\n",
    "from utils import *\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from collections.abc import Mapping\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "# ========= User Config =========\n",
    "GRAPH_PATH = \"../wind_and_graph_2024/GULF_VISITS_cargo_tanker_2024_merged.gexf\"\n",
    "WIND_MAP_PATH = \"../wind_and_graph_2024/2024_august_wind_data.csv\"\n",
    "\n",
    "MODEL_PATHS = [\n",
    "    \"model_step_10000000_4.zip\",\n",
    "    \"model_step_10000000_42.zip\",\n",
    "    \"model_step_10000000_31.zip\",\n",
    "]\n",
    "\n",
    "CSV_DIR = \"eval_csvs\"  # per-model CSVs will be stored here\n",
    "MERGED_CSV_OUT = os.path.join(CSV_DIR, \"evaluation_info_logs_all_models.csv\")\n",
    "\n",
    "PAIR_LIST = [\n",
    "    (\"861ab6847ffffff\", \"860e4daafffffff\"),\n",
    "]\n",
    "\n",
    "EPISODES = 5\n",
    "MAX_STEPS = 1000\n",
    "H3_RES = 6\n",
    "WIND_THRESHOLD = 22\n",
    "RENDER_MODE = \"human\"\n",
    "\n",
    "# ========= Helpers =========\n",
    "def flatten_dict(d, parent_key=\"\", sep=\".\"):\n",
    "    \"\"\"Flatten nested dictionaries using dot notation for keys.\"\"\"\n",
    "    items = []\n",
    "    for k, v in (d or {}).items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else str(k)\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def model_tag_from_path(path):\n",
    "    \"\"\"Return a short tag from the model filename (e.g., 'model_step_10000000_42').\"\"\"\n",
    "    return os.path.splitext(os.path.basename(path))\n",
    "\n",
    "# ========= Load graph and wind map =========\n",
    "G_visits = nx.read_gexf(GRAPH_PATH).to_undirected()\n",
    "try:\n",
    "    full_wind_map = load_full_wind_map(WIND_MAP_PATH)  # Provided by project\n",
    "except NameError:\n",
    "    # Fallback: raw DataFrame if helper not in scope (works if MariNav accepts it)\n",
    "    full_wind_map = pd.read_csv(WIND_MAP_PATH)\n",
    "\n",
    "# ========= Evaluation routine for a single model =========\n",
    "def evaluate_one_model(model_path):\n",
    "    \"\"\"\n",
    "    Run EPISODES for one model checkpoint, write a per-model CSV, and return per-episode returns.\n",
    "    \"\"\"\n",
    "    # Create fresh environment for each model\n",
    "    try:\n",
    "        eval_env = MariNav(\n",
    "            pairs=PAIR_LIST,\n",
    "            graph=G_visits,\n",
    "            wind_map=full_wind_map,\n",
    "            h3_resolution=H3_RES,\n",
    "            wind_threshold=WIND_THRESHOLD,\n",
    "            render_mode=RENDER_MODE,\n",
    "        )\n",
    "    except NameError as e:\n",
    "        raise RuntimeError(\n",
    "            \"MariNav class is not available in current environment. Import it before running.\"\n",
    "        ) from e\n",
    "\n",
    "    # Load model with this env\n",
    "    model = MaskablePPO.load(model_path, env=eval_env)\n",
    "\n",
    "    rows = []\n",
    "    episode_returns = []\n",
    "    model_tag = model_tag_from_path(model_path)\n",
    "\n",
    "    print(f\"\\n===== Evaluating {model_tag} =====\")\n",
    "    for episode in range(1, EPISODES + 1):\n",
    "        obs, _ = eval_env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        agent_path = [getattr(eval_env, \"current_h3\", None)]\n",
    "        start_h3 = getattr(eval_env, \"start_h3\", None)\n",
    "        goal_h3 = getattr(eval_env, \"goal_h3\", None)\n",
    "        print(f\"\\nðŸš€ Starting Episode {episode} from H3 {start_h3} to {goal_h3} | model={model_tag}\")\n",
    "\n",
    "        ep_ret = 0.0\n",
    "        for step in range(MAX_STEPS):\n",
    "            # Predict using action masks from env (required for MaskablePPO)\n",
    "            action_masks = eval_env.action_masks()\n",
    "            action, _ = model.predict(obs, deterministic=False, action_masks=action_masks)\n",
    "            obs, reward, done, truncated, info = eval_env.step(action)\n",
    "            ep_ret += float(reward)\n",
    "\n",
    "            # Build base record\n",
    "            rec = OrderedDict()\n",
    "            rec[\"timestamp\"] = time.time()\n",
    "            rec[\"model_tag\"] = model_tag\n",
    "            rec[\"episode\"] = episode\n",
    "            rec[\"step\"] = step\n",
    "            rec[\"start_h3\"] = start_h3\n",
    "            rec[\"goal_h3\"] = goal_h3\n",
    "            rec[\"current_h3\"] = getattr(eval_env, \"current_h3\", None)\n",
    "\n",
    "            # Normalize action\n",
    "            if np.isscalar(action) or np.array(action).size == 1:\n",
    "                try:\n",
    "                    rec[\"action\"] = int(action)\n",
    "                except Exception:\n",
    "                    rec[\"action\"] = float(np.array(action).item())\n",
    "            else:\n",
    "                rec[\"action\"] = json.dumps(np.array(action).tolist())\n",
    "\n",
    "            rec[\"reward\"] = float(reward)\n",
    "            rec[\"done\"] = bool(done)\n",
    "            rec[\"truncated\"] = bool(truncated)\n",
    "\n",
    "            # Lat/Lon for current H3 if available\n",
    "            try:\n",
    "                lat, lon = h3.cell_to_latlng(rec[\"current_h3\"]) if rec[\"current_h3\"] else (None, None)\n",
    "            except Exception:\n",
    "                lat, lon = (None, None)\n",
    "            rec[\"lat\"] = lat\n",
    "            rec[\"lon\"] = lon\n",
    "\n",
    "            # Flatten info and merge\n",
    "            info_flat = flatten_dict(info if isinstance(info, dict) else {})\n",
    "            full_record = {**rec, **info_flat}\n",
    "            rows.append(full_record)\n",
    "\n",
    "            # Optional console print for checkpoints\n",
    "            if step % 1000 == 0 or done:\n",
    "                speed = info_flat.get(\"speed\", -1.0)\n",
    "                wind_dir = info_flat.get(\"wind_direction\", -1.0)\n",
    "                try:\n",
    "                    speed_val = float(speed) if speed is not None else -1.0\n",
    "                except Exception:\n",
    "                    speed_val = -1.0\n",
    "                try:\n",
    "                    wind_val = float(wind_dir) if wind_dir is not None else -1.0\n",
    "                except Exception:\n",
    "                    wind_val = -1.0\n",
    "                print(\n",
    "                    f\"ðŸ§­ Step {step} | H3: {rec['current_h3']} | \"\n",
    "                    f\"Speed: {speed_val:.2f} knots | Wind Dir: {wind_val:.2f} | Reward: {reward:.2f}\"\n",
    "                )\n",
    "                print(f\"info: {info}\")\n",
    "\n",
    "            agent_path.append(rec[\"current_h3\"])\n",
    "\n",
    "            if truncated or done:\n",
    "                break\n",
    "\n",
    "        episode_returns.append(ep_ret)\n",
    "        print(f\"âœ… Episode {episode} return: {ep_ret:.6f} | model={model_tag}\")\n",
    "\n",
    "    # ========= Write per-model CSV =========\n",
    "    os.makedirs(CSV_DIR, exist_ok=True)\n",
    "    # Consolidate all encountered columns (stable ordering)\n",
    "    all_cols = []\n",
    "    for r in rows:\n",
    "        for k in r.keys():\n",
    "            if k not in all_cols:\n",
    "                all_cols.append(k)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=all_cols)\n",
    "    per_model_csv = os.path.join(CSV_DIR, f\"evaluation_info_logs_{model_tag}.csv\")\n",
    "    df.to_csv(per_model_csv, index=False)\n",
    "    print(f\"Wrote {len(df)} rows to {per_model_csv}\")\n",
    "\n",
    "    return episode_returns, per_model_csv\n",
    "\n",
    "# ========= Run all models, merge CSVs, and print averaged final return =========\n",
    "all_episode_returns = {}\n",
    "all_csv_paths = []\n",
    "\n",
    "for mp in MODEL_PATHS:\n",
    "    ep_returns, csv_path = evaluate_one_model(mp)\n",
    "    all_episode_returns[model_tag_from_path(mp)] = ep_returns\n",
    "    all_csv_paths.append(csv_path)\n",
    "\n",
    "# Averaged final episode return across models\n",
    "final_returns = [ret_list[-1] for ret_list in all_episode_returns.values() if len(ret_list) > 0]\n",
    "avg_final_return = float(np.mean(final_returns)) if len(final_returns) > 0 else float(\"nan\")\n",
    "print(\"\\n================ Summary ================\")\n",
    "for tag, rets in all_episode_returns.items():\n",
    "    print(f\"{tag}: episode returns = {np.array(rets).round(6).tolist()}\")\n",
    "print(f\"\\nAveraged final episode return across {len(final_returns)} models: {avg_final_return:.6f}\")\n",
    "\n",
    "# Optional: merge per-model CSVs into one\n",
    "try:\n",
    "    merged_frames = []\n",
    "    for p in all_csv_paths:\n",
    "        df_ = pd.read_csv(p)\n",
    "        merged_frames.append(df_)\n",
    "    if merged_frames:\n",
    "        merged_df = pd.concat(merged_frames, ignore_index=True)\n",
    "        merged_df.to_csv(MERGED_CSV_OUT, index=False)\n",
    "        print(f\"Wrote merged CSV with {len(merged_df)} rows to {MERGED_CSV_OUT}\")\n",
    "except Exception as e:\n",
    "    print(f\"Skipping merged CSV due to error: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
